{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "omSpeech",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNMVWeO7k2UbuTvEp2KCLKh"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ti9kxLFehHyi",
        "colab_type": "text"
      },
      "source": [
        "OM **IPYNB**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZCDfyDlnqpn",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAOlSXPlnsGY",
        "colab_type": "text"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXwBSEyahpAq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install Keras==2.2.0\n",
        "!pip install pandas==0.22.0\n",
        "!pip install pandas-ml==0.5.0\n",
        "!pip install tensorflow>=1.14.0\n",
        "!pip install tensorflow-gpu>=1.14.0\n",
        "!pip install wget==3.2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23KHWyI2prRe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "outputId": "ef431fa5-a2ec-4f13-9cd0-ca44f952dfc6"
      },
      "source": [
        "!pip install Keras==2.2.0"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Keras==2.2.0\n",
            "  Using cached https://files.pythonhosted.org/packages/68/12/4cabc5c01451eb3b413d19ea151f36e33026fc0efb932bf51bcaf54acbf5/Keras-2.2.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.0) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.0) (1.12.0)\n",
            "Collecting keras-applications==1.0.2\n",
            "  Using cached https://files.pythonhosted.org/packages/e2/60/c557075e586e968d7a9c314aa38c236b37cb3ee6b37e8d57152b1a5e0b47/Keras_Applications-1.0.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.0) (1.18.4)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.0) (1.4.1)\n",
            "Collecting keras-preprocessing==1.0.1\n",
            "  Using cached https://files.pythonhosted.org/packages/f8/33/275506afe1d96b221f66f95adba94d1b73f6b6087cfb6132a5655b6fe338/Keras_Preprocessing-1.0.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.0) (2.10.0)\n",
            "\u001b[31mERROR: tensorflow 2.2.0 has requirement keras-preprocessing>=1.1.0, but you'll have keras-preprocessing 1.0.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-gpu 2.2.0 has requirement keras-preprocessing>=1.1.0, but you'll have keras-preprocessing 1.0.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: keras-applications, keras-preprocessing, Keras\n",
            "  Found existing installation: Keras-Applications 1.0.8\n",
            "    Uninstalling Keras-Applications-1.0.8:\n",
            "      Successfully uninstalled Keras-Applications-1.0.8\n",
            "  Found existing installation: Keras-Preprocessing 1.1.0\n",
            "    Uninstalling Keras-Preprocessing-1.1.0:\n",
            "      Successfully uninstalled Keras-Preprocessing-1.1.0\n",
            "  Found existing installation: Keras 2.3.1\n",
            "    Uninstalling Keras-2.3.1:\n",
            "      Successfully uninstalled Keras-2.3.1\n",
            "Successfully installed Keras-2.2.0 keras-applications-1.0.2 keras-preprocessing-1.0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras",
                  "keras_applications",
                  "keras_preprocessing"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oe7jIsE-ohzC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9584d68e-3975-499f-adf9-9fa7e97863ac"
      },
      "source": [
        "import keras as pdml\n",
        "\n",
        "print(pdml.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXMafUz-gNpZ",
        "colab_type": "text"
      },
      "source": [
        "#Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pZB4LPPef_D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import wget\n",
        "import tarfile\n",
        "\n",
        "from shutil import rmtree\n",
        "\n",
        "DATASET_URL = 'http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz'\n",
        "ARCHIVE = os.path.basename(DATASET_URL)\n",
        "\n",
        "wget.download(DATASET_URL)\n",
        "\n",
        "if os.path.exists('data'):\n",
        "  rmtree('data')\n",
        "\n",
        "os.makedirs('data/train')\n",
        "\n",
        "with tarfile.open(ARCHIVE, 'r:gz') as tar:\n",
        "  tar.extractall(path='data/train')\n",
        "\n",
        "os.remove(ARCHIVE)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPMBLy99gseu",
        "colab_type": "text"
      },
      "source": [
        "#Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M92q9DYPfm7g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import sklearn.impute\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#from pandas_ml import ConfusionMatrix\n",
        "from keras.callbacks import Callback\n",
        "\n",
        "\n",
        "def log_loss(y_true, y_pred, eps=1e-12):\n",
        "  y_pred = np.clip(y_pred, eps, 1. - eps)\n",
        "  ce = -(np.sum(y_true * np.log(y_pred), axis=1))\n",
        "  mce = ce.mean()\n",
        "  return mce\n",
        "\n",
        "\n",
        "class ConfusionMatrixCallback(Callback):\n",
        "\n",
        "  def __init__(self, validation_data, validation_steps, wanted_words, all_words,\n",
        "               label2int):\n",
        "    self.validation_data = validation_data\n",
        "    self.validation_steps = validation_steps\n",
        "    self.wanted_words = wanted_words\n",
        "    self.all_words = all_words\n",
        "    self.label2int = label2int\n",
        "    self.int2label = {v: k for k, v in label2int.items()}\n",
        "    with open('confusion_matrix.txt', 'w'):\n",
        "      pass\n",
        "    with open('wanted_confusion_matrix.txt', 'w'):\n",
        "      pass\n",
        "\n",
        "  def accuracies(self, confusion_val):\n",
        "    accuracies = []\n",
        "    for i in range(confusion_val.shape[0]):\n",
        "      num = confusion_val[i, :].sum()\n",
        "      if num:\n",
        "        accuracies.append(confusion_val[i, i] / num)\n",
        "      else:\n",
        "        accuracies.append(0.0)\n",
        "    accuracies = np.float32(accuracies)\n",
        "    return accuracies\n",
        "\n",
        "  def accuracy(self, confusion_val):\n",
        "    num_correct = 0\n",
        "    for i in range(confusion_val.shape[0]):\n",
        "      num_correct += confusion_val[i, i]\n",
        "    accuracy = float(num_correct) / confusion_val.sum()\n",
        "    return accuracy\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    y_true, y_pred = [], []\n",
        "    for i in range(self.validation_steps):\n",
        "      X_batch, y_true_batch = next(self.validation_data)\n",
        "      y_pred_batch = self.model.predict(X_batch)\n",
        "\n",
        "      y_true.extend(y_true_batch)\n",
        "      y_pred.extend(y_pred_batch)\n",
        "\n",
        "    y_true = np.float32(y_true)\n",
        "    y_pred = np.float32(y_pred)\n",
        "    val_loss = log_loss(y_true, y_pred)\n",
        "    # map integer labels to strings\n",
        "    y_true = list(y_true.argmax(axis=-1))\n",
        "    y_pred = list(y_pred.argmax(axis=-1))\n",
        "    y_true = [self.int2label[y] for y in y_true]\n",
        "    y_pred = [self.int2label[y] for y in y_pred]\n",
        "    confusion = ConfusionMatrix(y_true, y_pred)\n",
        "    accs = self.accuracies(confusion._df_confusion.values)\n",
        "    acc = self.accuracy(confusion._df_confusion.values)\n",
        "    # same for wanted words\n",
        "    y_true = [y if y in self.wanted_words else '_unknown_' for y in y_true]\n",
        "    y_pred = [y if y in self.wanted_words else '_unknown_' for y in y_pred]\n",
        "    wanted_words_confusion = ConfusionMatrix(y_true, y_pred)\n",
        "    wanted_accs = self.accuracies(wanted_words_confusion._df_confusion.values)\n",
        "    acc_line = ('\\n[%03d]: val_categorical_accuracy: %.2f, '\n",
        "                'val_mean_categorical_accuracy_wanted: %.2f') % (\n",
        "                    epoch, acc, wanted_accs.mean())  # noqa\n",
        "    with open('confusion_matrix.txt', 'a') as f:\n",
        "      f.write('%s\\n' % acc_line)\n",
        "      f.write(confusion.to_dataframe().to_string())\n",
        "\n",
        "    with open('wanted_confusion_matrix.txt', 'a') as f:\n",
        "      f.write('%s\\n' % acc_line)\n",
        "      f.write(wanted_words_confusion.to_dataframe().to_string())\n",
        "\n",
        "    logs['val_loss'] = val_loss\n",
        "    logs['val_categorical_accuracy'] = acc\n",
        "    logs['val_mean_categorical_accuracy_all'] = accs.mean()\n",
        "    logs['val_mean_categorical_accuracy_wanted'] = wanted_accs.mean()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBNlC5_kg1gw",
        "colab_type": "text"
      },
      "source": [
        "#Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRr-2YPDf93d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "\n",
        "def data_gen(audio_processor,\n",
        "             sess,\n",
        "             batch_size=128,\n",
        "             background_frequency=0.3,\n",
        "             background_volume_range=0.15,\n",
        "             foreground_frequency=0.3,\n",
        "             foreground_volume_range=0.15,\n",
        "             time_shift_frequency=0.3,\n",
        "             time_shift_range=[-500, 0],\n",
        "             mode='validation',\n",
        "             flip_frequency=0.0,\n",
        "             silence_volume_range=0.3):\n",
        "  ep_count = 0\n",
        "  offset = 0\n",
        "  if mode != 'training':\n",
        "    background_frequency = 0.0\n",
        "    background_volume_range = 0.0\n",
        "    foreground_frequency = 0.0\n",
        "    foreground_volume_range = 0.0\n",
        "    time_shift_frequency = 0.0\n",
        "    time_shift_range = [0, 0]\n",
        "    flip_frequency = 0.0\n",
        "    # silence_volume_range: stays the same for validation\n",
        "  while True:\n",
        "    X, y = audio_processor.get_data(\n",
        "        how_many=batch_size,\n",
        "        offset=0 if mode == 'training' else offset,\n",
        "        background_frequency=background_frequency,\n",
        "        background_volume_range=background_volume_range,\n",
        "        foreground_frequency=foreground_frequency,\n",
        "        foreground_volume_range=foreground_volume_range,\n",
        "        time_shift_frequency=time_shift_frequency,\n",
        "        time_shift_range=time_shift_range,\n",
        "        mode=mode,\n",
        "        sess=sess,\n",
        "        flip_frequency=flip_frequency,\n",
        "        silence_volume_range=silence_volume_range)\n",
        "    offset += batch_size\n",
        "    if offset > audio_processor.set_size(mode) - batch_size:\n",
        "      offset = 0\n",
        "      print('\\n[Ep:%03d: %s-mode]' % (ep_count, mode))\n",
        "      ep_count += 1\n",
        "    yield X, y\n",
        "\n",
        "\n",
        "def tf_roll(a, shift, a_len=16000):\n",
        "  # https://stackoverflow.com/questions/42651714/vector-shift-roll-in-tensorflow\n",
        "  def roll_left(a, shift, a_len):\n",
        "    shift %= a_len\n",
        "    rolled = tf.concat([a[a_len - shift:, :], a[:a_len - shift, :]], axis=0)\n",
        "    return rolled\n",
        "\n",
        "  def roll_right(a, shift, a_len):\n",
        "    shift = -shift\n",
        "    shift %= a_len\n",
        "    rolled = tf.concat([a[shift:, :], a[:shift, :]], axis=0)\n",
        "    return rolled\n",
        "\n",
        "  # https://stackoverflow.com/questions/35833011/how-to-add-if-condition-in-a-tensorflow-graph\n",
        "  return tf.cond(\n",
        "      tf.greater_equal(shift, 0),\n",
        "      true_fn=lambda: roll_left(a, shift, a_len),\n",
        "      false_fn=lambda: roll_right(a, shift, a_len))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iusshFuzgek1",
        "colab_type": "text"
      },
      "source": [
        "#Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXSQ06iPfz_0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import hashlib\n",
        "import math\n",
        "import os.path\n",
        "import random\n",
        "import re\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "MAX_NUM_WAVS_PER_CLASS = 2**27 - 1  # ~134M\n",
        "SILENCE_LABEL = '_silence_'\n",
        "SILENCE_INDEX = 0\n",
        "UNKNOWN_WORD_LABEL = '_unknown_'\n",
        "UNKNOWN_WORD_INDEX = 1\n",
        "BACKGROUND_NOISE_DIR_NAME = '_background_noise_'\n",
        "RANDOM_SEED = 59185\n",
        "\n",
        "\n",
        "def prepare_words_list(wanted_words):\n",
        "  \"\"\"Prepends common tokens to the custom word list.\"\"\"\n",
        "  return [SILENCE_LABEL, UNKNOWN_WORD_LABEL] + wanted_words\n",
        "\n",
        "\n",
        "def which_set(filename, validation_percentage, testing_percentage):\n",
        "  \"\"\"Determines which data partition the file should belong to.\"\"\"\n",
        "  dir_name = os.path.basename(os.path.dirname(filename))\n",
        "  if dir_name == 'unknown_unknown':\n",
        "    return 'training'\n",
        "\n",
        "  base_name = os.path.basename(filename)\n",
        "  hash_name = re.sub(r'_nohash_.*$', '', base_name)\n",
        "\n",
        "  hash_name_hashed = hashlib.sha1(tf.compat.as_bytes(hash_name)).hexdigest()\n",
        "  percentage_hash = ((int(hash_name_hashed, 16) % (MAX_NUM_WAVS_PER_CLASS + 1))\n",
        "                     * (100.0 / MAX_NUM_WAVS_PER_CLASS))\n",
        "  if percentage_hash < validation_percentage:\n",
        "    result = 'validation'\n",
        "  elif percentage_hash < (testing_percentage + validation_percentage):\n",
        "    result = 'testing'\n",
        "  else:\n",
        "    result = 'training'\n",
        "  return result\n",
        "\n",
        "\n",
        "def load_wav_file(filename):\n",
        "  \"\"\"Loads an audio file and returns a float PCM-encoded array of samples.\"\"\"\n",
        "  with tf.Session(graph=tf.Graph()) as sess:\n",
        "    wav_filename_placeholder = tf.placeholder(tf.string, [])\n",
        "    wav_loader = tf.io.read_file(wav_filename_placeholder)\n",
        "    wav_decoder = tf.audio.decode_wav(wav_loader, desired_channels=1)\n",
        "    return sess.run(\n",
        "        wav_decoder, feed_dict={\n",
        "            wav_filename_placeholder: filename\n",
        "        }).audio.flatten()\n",
        "\n",
        "\n",
        "def save_wav_file(filename, wav_data, sample_rate):\n",
        "  \"\"\"Saves audio sample data to a .wav audio file.\"\"\"\n",
        "  with tf.Session(graph=tf.Graph()) as sess:\n",
        "    wav_filename_placeholder = tf.placeholder(tf.string, [])\n",
        "    sample_rate_placeholder = tf.placeholder(tf.int32, [])\n",
        "    wav_data_placeholder = tf.placeholder(tf.float32, [None, 1])\n",
        "    wav_encoder = tf.audio.encode_wav(wav_data_placeholder,\n",
        "                                      sample_rate_placeholder)\n",
        "    wav_saver = tf.io.write_file(wav_filename_placeholder, wav_encoder)\n",
        "    sess.run(\n",
        "        wav_saver,\n",
        "        feed_dict={\n",
        "            wav_filename_placeholder: filename,\n",
        "            sample_rate_placeholder: sample_rate,\n",
        "            wav_data_placeholder: np.reshape(wav_data, (-1, 1))\n",
        "        })\n",
        "\n",
        "\n",
        "class AudioProcessor(object):\n",
        "  \"\"\"Handles loading, partitioning, and preparing audio training data.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               data_dirs,\n",
        "               silence_percentage,\n",
        "               unknown_percentage,\n",
        "               wanted_words,\n",
        "               validation_percentage,\n",
        "               testing_percentage,\n",
        "               model_settings,\n",
        "               output_representation=False):\n",
        "    self.data_dirs = data_dirs\n",
        "    assert output_representation in {'raw', 'spec', 'mfcc', 'mfcc_and_raw'}\n",
        "    self.output_representation = output_representation\n",
        "    self.model_settings = model_settings\n",
        "    for data_dir in self.data_dirs:\n",
        "      self.maybe_download_and_extract_dataset(data_dir)\n",
        "    self.prepare_data_index(silence_percentage, unknown_percentage,\n",
        "                            wanted_words, validation_percentage,\n",
        "                            testing_percentage)\n",
        "    self.prepare_background_data()\n",
        "    self.prepare_processing_graph(model_settings)\n",
        "\n",
        "  def maybe_download_and_extract_dataset(self, data_dir):\n",
        "    if not os.path.exists(data_dir):\n",
        "      print('Please download the dataset!')\n",
        "      sys.exit(0)\n",
        "\n",
        "  def prepare_data_index(self, silence_percentage, unknown_percentage,\n",
        "                         wanted_words, validation_percentage,\n",
        "                         testing_percentage):\n",
        "    \"\"\"Prepares a list of the samples organized by set and label.\"\"\"\n",
        "    random.seed(RANDOM_SEED)\n",
        "    wanted_words_index = {}\n",
        "    for index, wanted_word in enumerate(wanted_words):\n",
        "      wanted_words_index[wanted_word] = index + 2\n",
        "    self.data_index = {'validation': [], 'testing': [], 'training': []}\n",
        "    unknown_index = {'validation': [], 'testing': [], 'training': []}\n",
        "    all_words = {}\n",
        "    # Look through all the subfolders to find audio samples\n",
        "    for data_dir in self.data_dirs:\n",
        "      search_path = os.path.join(data_dir, '*', '*.wav')\n",
        "      for wav_path in tf.io.gfile.glob(search_path):\n",
        "        word = re.search('.*/([^/]+)/.*.wav', wav_path).group(1).lower()\n",
        "        # Treat the '_background_noise_' folder as a special case,\n",
        "        # since we expect it to contain long audio samples we mix in\n",
        "        # to improve training.\n",
        "        if word == BACKGROUND_NOISE_DIR_NAME:\n",
        "          continue\n",
        "        all_words[word] = True\n",
        "        set_index = which_set(wav_path, validation_percentage,\n",
        "                              testing_percentage)\n",
        "        # If it's a known class, store its detail, otherwise add it to the list\n",
        "        # we'll use to train the unknown label.\n",
        "        if word in wanted_words_index:\n",
        "          self.data_index[set_index].append({'label': word, 'file': wav_path})\n",
        "        else:\n",
        "          unknown_index[set_index].append({'label': word, 'file': wav_path})\n",
        "      if not all_words:\n",
        "        raise Exception('No .wavs found at ' + search_path)\n",
        "      for index, wanted_word in enumerate(wanted_words):\n",
        "        if wanted_word not in all_words:\n",
        "          raise Exception('Expected to find ' + wanted_word +\n",
        "                          ' in labels but only found ' +\n",
        "                          ', '.join(all_words.keys()))\n",
        "    # We need an arbitrary file to load as the input for the silence samples.\n",
        "    # It's multiplied by zero later, so the content doesn't matter.\n",
        "    silence_wav_path = self.data_index['training'][0]['file']\n",
        "    for set_index in ['validation', 'testing', 'training']:\n",
        "      set_size = len(self.data_index[set_index])\n",
        "      silence_size = int(math.ceil(set_size * silence_percentage / 100))\n",
        "      for _ in range(silence_size):\n",
        "        self.data_index[set_index].append({\n",
        "            'label': SILENCE_LABEL,\n",
        "            'file': silence_wav_path\n",
        "        })\n",
        "      # Pick some unknowns to add to each partition of the data set.\n",
        "      random.shuffle(unknown_index[set_index])\n",
        "      unknown_size = int(math.ceil(set_size * unknown_percentage / 100))\n",
        "      self.data_index[set_index].extend(unknown_index[set_index][:unknown_size])\n",
        "    # Make sure the ordering is random.\n",
        "    for set_index in ['validation', 'testing', 'training']:\n",
        "      # not really needed since the indices are chosen by random\n",
        "      random.shuffle(self.data_index[set_index])\n",
        "    # Prepare the rest of the result data structure.\n",
        "    self.words_list = prepare_words_list(wanted_words)\n",
        "    self.word_to_index = {}\n",
        "    for word in all_words:\n",
        "      if word in wanted_words_index:\n",
        "        self.word_to_index[word] = wanted_words_index[word]\n",
        "      else:\n",
        "        self.word_to_index[word] = UNKNOWN_WORD_INDEX\n",
        "    self.word_to_index[SILENCE_LABEL] = SILENCE_INDEX\n",
        "\n",
        "  def prepare_background_data(self):\n",
        "    \"\"\"Searches a folder for background noise audio and loads it into memory.\"\"\"\n",
        "    self.background_data = []\n",
        "    background_dir = os.path.join(self.data_dirs[0], BACKGROUND_NOISE_DIR_NAME)\n",
        "    if not os.path.exists(background_dir):\n",
        "      return self.background_data\n",
        "    with tf.Session(graph=tf.Graph()) as sess:\n",
        "      wav_filename_placeholder = tf.placeholder(tf.string, [])\n",
        "      wav_loader = tf.io.read_file(wav_filename_placeholder)\n",
        "      wav_decoder = tf.audio.decode_wav(wav_loader, desired_channels=1)\n",
        "      search_path = os.path.join(self.data_dirs[0], BACKGROUND_NOISE_DIR_NAME,\n",
        "                                 '*.wav')\n",
        "      for wav_path in tf.io.gfile.glob(search_path):\n",
        "        wav_data = sess.run(\n",
        "            wav_decoder, feed_dict={\n",
        "                wav_filename_placeholder: wav_path\n",
        "            }).audio.flatten()\n",
        "        self.background_data.append(wav_data)\n",
        "      if not self.background_data:\n",
        "        raise Exception('No background wav files were found in ' + search_path)\n",
        "\n",
        "  def prepare_processing_graph(self, model_settings):\n",
        "    \"\"\"Builds a TensorFlow graph to apply the input distortions.\"\"\"\n",
        "    desired_samples = model_settings['desired_samples']\n",
        "    self.wav_filename_placeholder_ = tf.placeholder(\n",
        "        tf.string, [], name='filename')\n",
        "    wav_loader = tf.io.read_file(self.wav_filename_placeholder_)\n",
        "    wav_decoder = tf.audio.decode_wav(\n",
        "        wav_loader, desired_channels=1, desired_samples=desired_samples)\n",
        "    # Allow the audio sample's volume to be adjusted.\n",
        "    self.foreground_volume_placeholder_ = tf.placeholder(\n",
        "        tf.float32, [], name='foreground_volme')\n",
        "    scaled_foreground = tf.multiply(wav_decoder.audio,\n",
        "                                    self.foreground_volume_placeholder_)\n",
        "    # Shift the sample's start position, and pad any gaps with zeros.\n",
        "    self.time_shift_placeholder_ = tf.placeholder(tf.int32, name='timeshift')\n",
        "    shifted_foreground = tf_roll(scaled_foreground,\n",
        "                                 self.time_shift_placeholder_)\n",
        "    # Mix in background noise.\n",
        "    self.background_data_placeholder_ = tf.placeholder(\n",
        "        tf.float32, [desired_samples, 1], name='background_data')\n",
        "    self.background_volume_placeholder_ = tf.placeholder(\n",
        "        tf.float32, [], name='background_volume')\n",
        "    background_mul = tf.multiply(self.background_data_placeholder_,\n",
        "                                 self.background_volume_placeholder_)\n",
        "    background_add = tf.add(background_mul, shifted_foreground)\n",
        "    # removed clipping: tf.clip_by_value(background_add, -1.0, 1.0)\n",
        "    self.background_clamp_ = background_add\n",
        "    self.background_clamp_ = tf.reshape(self.background_clamp_,\n",
        "                                        (1, model_settings['desired_samples']))\n",
        "    # Run the spectrogram and MFCC ops to get a 2D 'fingerprint' of the audio.\n",
        "    stfts = tf.signal.stft(\n",
        "        self.background_clamp_,\n",
        "        frame_length=model_settings['window_size_samples'],\n",
        "        frame_step=model_settings['window_stride_samples'],\n",
        "        fft_length=None)\n",
        "    self.spectrogram_ = tf.abs(stfts)\n",
        "    num_spectrogram_bins = self.spectrogram_.shape[-1].value\n",
        "    lower_edge_hertz, upper_edge_hertz = 80.0, 7600.0\n",
        "    linear_to_mel_weight_matrix = \\\n",
        "        tf.signal.linear_to_mel_weight_matrix(\n",
        "            model_settings['dct_coefficient_count'],\n",
        "            num_spectrogram_bins, model_settings['sample_rate'],\n",
        "            lower_edge_hertz, upper_edge_hertz)\n",
        "    mel_spectrograms = tf.tensordot(self.spectrogram_,\n",
        "                                    linear_to_mel_weight_matrix, 1)\n",
        "    mel_spectrograms.set_shape(self.spectrogram_.shape[:-1].concatenate(\n",
        "        linear_to_mel_weight_matrix.shape[-1:]))\n",
        "    log_mel_spectrograms = tf.log(mel_spectrograms + 1e-6)\n",
        "    self.mfcc_ = tf.signal.mfccs_from_log_mel_spectrograms(\n",
        "        log_mel_spectrograms)[:, :, :\n",
        "                              model_settings['num_log_mel_features']]  # :13\n",
        "\n",
        "  def set_size(self, mode):\n",
        "    \"\"\"Calculates the number of samples in the dataset partition.\"\"\"\n",
        "    return len(self.data_index[mode])\n",
        "\n",
        "  def get_data(self,\n",
        "               how_many,\n",
        "               offset,\n",
        "               background_frequency,\n",
        "               background_volume_range,\n",
        "               foreground_frequency,\n",
        "               foreground_volume_range,\n",
        "               time_shift_frequency,\n",
        "               time_shift_range,\n",
        "               mode,\n",
        "               sess,\n",
        "               flip_frequency=0.0,\n",
        "               silence_volume_range=0.0):\n",
        "    \"\"\"Gather samples from the data set, applying transformations as needed.\"\"\"\n",
        "    # Pick one of the partitions to choose samples from.\n",
        "    model_settings = self.model_settings\n",
        "    candidates = self.data_index[mode]\n",
        "    if how_many == -1:\n",
        "      sample_count = len(candidates)\n",
        "    else:\n",
        "      sample_count = max(0, min(how_many, len(candidates) - offset))\n",
        "    # Data and labels will be populated and returned.\n",
        "    if self.output_representation == 'raw':\n",
        "      data_dim = model_settings['desired_samples']\n",
        "    elif self.output_representation == 'spec':\n",
        "      data_dim = model_settings['spectrogram_length'] * model_settings[\n",
        "          'spectrogram_frequencies']\n",
        "    elif self.output_representation == 'mfcc':\n",
        "      data_dim = model_settings['spectrogram_length'] * \\\n",
        "                 model_settings['num_log_mel_features']\n",
        "    elif self.output_representation == 'mfcc_and_raw':\n",
        "      data_dim = model_settings['spectrogram_length'] * \\\n",
        "                 model_settings['num_log_mel_features']\n",
        "      raw_data = np.zeros((sample_count, model_settings['desired_samples']))\n",
        "\n",
        "    data = np.zeros((sample_count, data_dim))\n",
        "    labels = np.zeros((sample_count, model_settings['label_count']))\n",
        "    desired_samples = model_settings['desired_samples']\n",
        "    use_background = self.background_data and (mode == 'training')\n",
        "    pick_deterministically = (mode != 'training')\n",
        "    # Use the processing graph we created earlier to repeatedly to generate the\n",
        "    # final output sample data we'll use in training.\n",
        "    for i in xrange(offset, offset + sample_count):\n",
        "      # Pick which audio sample to use.\n",
        "      if how_many == -1 or pick_deterministically:\n",
        "        sample_index = i\n",
        "        sample = candidates[sample_index]\n",
        "      else:\n",
        "        sample_index = np.random.randint(len(candidates))\n",
        "        sample = candidates[sample_index]\n",
        "\n",
        "      # If we're time shifting, set up the offset for this sample.\n",
        "      if np.random.uniform(0.0, 1.0) < time_shift_frequency:\n",
        "        time_shift = np.random.randint(time_shift_range[0],\n",
        "                                       time_shift_range[1] + 1)\n",
        "      else:\n",
        "        time_shift = 0\n",
        "      input_dict = {\n",
        "          self.wav_filename_placeholder_: sample['file'],\n",
        "          self.time_shift_placeholder_: time_shift,\n",
        "      }\n",
        "      # Choose a section of background noise to mix in.\n",
        "      if use_background:\n",
        "        background_index = np.random.randint(len(self.background_data))\n",
        "        background_samples = self.background_data[background_index]\n",
        "        background_offset = np.random.randint(\n",
        "            0,\n",
        "            len(background_samples) - model_settings['desired_samples'])\n",
        "        background_clipped = background_samples[background_offset:(\n",
        "            background_offset + desired_samples)]\n",
        "        background_reshaped = background_clipped.reshape([desired_samples, 1])\n",
        "        if np.random.uniform(0, 1) < background_frequency:\n",
        "          background_volume = np.random.uniform(0, background_volume_range)\n",
        "        else:\n",
        "          background_volume = 0.0\n",
        "          # silence class with all zeros is boring!\n",
        "          if sample['label'] == SILENCE_LABEL and \\\n",
        "                  np.random.uniform(0, 1) < 0.9:\n",
        "            background_volume = np.random.uniform(0, silence_volume_range)\n",
        "      else:\n",
        "        background_reshaped = np.zeros([desired_samples, 1])\n",
        "        background_volume = 0.0\n",
        "      input_dict[self.background_data_placeholder_] = background_reshaped\n",
        "      input_dict[self.background_volume_placeholder_] = background_volume\n",
        "      # If we want silence, mute out the main sample but leave the background.\n",
        "      if sample['label'] == SILENCE_LABEL:\n",
        "        input_dict[self.foreground_volume_placeholder_] = 0.0\n",
        "      else:\n",
        "        # Turn it up or down\n",
        "        foreground_volume = 1.0\n",
        "        if np.random.uniform(0, 1) < foreground_frequency:\n",
        "          foreground_volume = 1.0 + np.random.uniform(-foreground_volume_range,\n",
        "                                                      foreground_volume_range)\n",
        "        # flip sign\n",
        "        if np.random.uniform(0, 1) < flip_frequency:\n",
        "          foreground_volume *= -1.0\n",
        "        input_dict[self.foreground_volume_placeholder_] = foreground_volume\n",
        "\n",
        "      # Run the graph to produce the output audio.\n",
        "      if self.output_representation == 'raw':\n",
        "        data[i - offset, :] = sess.run(\n",
        "            self.background_clamp_, feed_dict=input_dict).flatten()\n",
        "      elif self.output_representation == 'spec':\n",
        "        data[i - offset, :] = sess.run(\n",
        "            self.spectrogram_, feed_dict=input_dict).flatten()\n",
        "      elif self.output_representation == 'mfcc':\n",
        "        data[i - offset, :] = sess.run(\n",
        "            self.mfcc_, feed_dict=input_dict).flatten()\n",
        "      elif self.output_representation == 'mfcc_and_raw':\n",
        "        raw_val, mfcc_val = sess.run([self.background_clamp_, self.mfcc_],\n",
        "                                     feed_dict=input_dict)\n",
        "        data[i - offset, :] = mfcc_val.flatten()\n",
        "        raw_data[i - offset, :] = raw_val.flatten()\n",
        "\n",
        "      label_index = self.word_to_index[sample['label']]\n",
        "      labels[i - offset, label_index] = 1\n",
        "\n",
        "    if self.output_representation != 'mfcc_and_raw':\n",
        "      return data, labels\n",
        "    else:\n",
        "      return [data, raw_data], labels\n",
        "\n",
        "  def get_unprocessed_data(self, how_many, model_settings, mode):\n",
        "    \"\"\"Gets sample data without transformations.\"\"\"\n",
        "    candidates = self.data_index[mode]\n",
        "    if how_many == -1:\n",
        "      sample_count = len(candidates)\n",
        "    else:\n",
        "      sample_count = how_many\n",
        "    desired_samples = model_settings['desired_samples']\n",
        "    words_list = self.words_list\n",
        "    data = np.zeros((sample_count, desired_samples))\n",
        "    labels = []\n",
        "    with tf.Session(graph=tf.Graph()) as sess:\n",
        "      wav_filename_placeholder = tf.placeholder(tf.string, [], name='filename')\n",
        "      wav_loader = tf.io.read_file(wav_filename_placeholder)\n",
        "      wav_decoder = tf.audio.decode_wav(\n",
        "          wav_loader, desired_channels=1, desired_samples=desired_samples)\n",
        "      foreground_volume_placeholder = tf.placeholder(\n",
        "          tf.float32, [], name='foreground_volume')\n",
        "      scaled_foreground = tf.multiply(wav_decoder.audio,\n",
        "                                      foreground_volume_placeholder)\n",
        "      for i in range(sample_count):\n",
        "        if how_many == -1:\n",
        "          sample_index = i\n",
        "        else:\n",
        "          sample_index = np.random.randint(len(candidates))\n",
        "        sample = candidates[sample_index]\n",
        "        input_dict = {wav_filename_placeholder: sample['file']}\n",
        "        if sample['label'] == SILENCE_LABEL:\n",
        "          input_dict[foreground_volume_placeholder] = 0\n",
        "        else:\n",
        "          input_dict[foreground_volume_placeholder] = 1\n",
        "        data[i, :] = sess.run(scaled_foreground, feed_dict=input_dict).flatten()\n",
        "        label_index = self.word_to_index[sample['label']]\n",
        "        labels.append(words_list[label_index])\n",
        "    return data, labels\n",
        "\n",
        "  def summary(self):\n",
        "    \"\"\"Prints a summary of classes and label distributions.\"\"\"\n",
        "    set_counts = {}\n",
        "    print('There are %d classes.' % (len(self.word_to_index)))\n",
        "    print(\"1%% <-> %d samples in 'training'\" % int(\n",
        "        self.set_size('training') / 100))\n",
        "    for set_index in ['training', 'validation', 'testing']:\n",
        "      counts = {k: 0 for k in sorted(self.word_to_index.keys())}\n",
        "      num_total = self.set_size(set_index)\n",
        "      for data_point in self.data_index[set_index]:\n",
        "        counts[data_point['label']] += (1.0 / num_total) * 100.0\n",
        "      set_counts[set_index] = counts\n",
        "\n",
        "    print('%-13s%-6s%-6s%-6s' % ('', 'Train', 'Val', 'Test'))\n",
        "    for label_name in sorted(\n",
        "        self.word_to_index.keys(), key=self.word_to_index.get):\n",
        "      line = '%02d %-12s: ' % (self.word_to_index[label_name], label_name)\n",
        "      for set_index in ['training', 'validation', 'testing']:\n",
        "        line += '%.1f%% ' % (set_counts[set_index][label_name])\n",
        "      print(line)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OU7bCnjigYiM",
        "colab_type": "text"
      },
      "source": [
        "#Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6Tr_l9Wfn4S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "def get_classes(wanted_only=False):\n",
        "  if wanted_only:\n",
        "    classes = 'stop down off right up go on yes left no'\n",
        "    classes = classes.split(' ')\n",
        "    assert len(classes) == 10\n",
        "  else:\n",
        "    classes = ('sheila nine stop bed four six down bird marvin cat off right '\n",
        "               'seven eight up three happy go zero on wow dog yes five one tree'\n",
        "               ' house two left no')  # noqa\n",
        "    classes = classes.split(' ')\n",
        "    assert len(classes) == 30\n",
        "  return classes\n",
        "\n",
        "\n",
        "def get_int2label(wanted_only=False, extend_reversed=False):\n",
        "  classes = get_classes(\n",
        "      wanted_only=wanted_only, extend_reversed=extend_reversed)\n",
        "  classes = prepare_words_list(classes)\n",
        "  int2label = {i: l for i, l in enumerate(classes)}\n",
        "  int2label = OrderedDict(sorted(int2label.items(), key=lambda x: x[0]))\n",
        "  return int2label\n",
        "\n",
        "\n",
        "def get_label2int(wanted_only=False, extend_reversed=False):\n",
        "  classes = get_classes(\n",
        "      wanted_only=wanted_only, extend_reversed=extend_reversed)\n",
        "  classes = prepare_words_list(classes)\n",
        "  label2int = {l: i for i, l in enumerate(classes)}\n",
        "  label2int = OrderedDict(sorted(label2int.items(), key=lambda x: x[1]))\n",
        "  return label2int\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQHFv2ZIgoGC",
        "colab_type": "text"
      },
      "source": [
        "#Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zz8C2uI3f4jD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "e75a1a42-72d4-4339-b7bb-6760e26c2a2f"
      },
      "source": [
        "import keras\n",
        "from keras.layers import *\n",
        "from keras.regularizers import l2\n",
        "from keras.models import Model\n",
        "\n",
        "\n",
        "def preprocess(x):\n",
        "  x = (x + 0.8) / 7.0\n",
        "  x = K.clip(x, -5, 5)\n",
        "  return x\n",
        "\n",
        "\n",
        "def preprocess_raw(x):\n",
        "  return x\n",
        "\n",
        "\n",
        "Preprocess = Lambda(preprocess)\n",
        "\n",
        "PreprocessRaw = Lambda(preprocess_raw)\n",
        "\n",
        "\n",
        "def relu6(x):\n",
        "  return K.relu(x, max_value=6)\n",
        "\n",
        "\n",
        "def conv_1d_time_stacked_model(input_size=16000, num_classes=11):\n",
        "  \"\"\" Creates a 1D model for temporal data.\n",
        "\n",
        "  Note: Use only\n",
        "  with compute_mfcc = False (e.g. raw waveform data).\n",
        "  Args:\n",
        "    input_size: How big the input vector is.\n",
        "    num_classes: How many classes are to be recognized.\n",
        "\n",
        "  Returns:\n",
        "    Compiled keras model\n",
        "  \"\"\"\n",
        "  input_layer = Input(shape=[input_size])\n",
        "  x = input_layer\n",
        "  x = Reshape([800, 20])(x)\n",
        "  x = PreprocessRaw(x)\n",
        "\n",
        "  def _reduce_conv(x, num_filters, k, strides=2, padding='valid'):\n",
        "    x = Conv1D(\n",
        "        num_filters,\n",
        "        k,\n",
        "        padding=padding,\n",
        "        use_bias=False,\n",
        "        kernel_regularizer=l2(0.00001))(\n",
        "            x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(relu6)(x)\n",
        "    x = MaxPool1D(pool_size=3, strides=strides, padding=padding)(x)\n",
        "    return x\n",
        "\n",
        "  def _context_conv(x, num_filters, k, dilation_rate=1, padding='valid'):\n",
        "    x = Conv1D(\n",
        "        num_filters,\n",
        "        k,\n",
        "        padding=padding,\n",
        "        dilation_rate=dilation_rate,\n",
        "        kernel_regularizer=l2(0.00001),\n",
        "        use_bias=False)(\n",
        "            x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(relu6)(x)\n",
        "    return x\n",
        "\n",
        "  x = _context_conv(x, 32, 1)\n",
        "  x = _reduce_conv(x, 48, 3)\n",
        "  x = _context_conv(x, 48, 3)\n",
        "  x = _reduce_conv(x, 96, 3)\n",
        "  x = _context_conv(x, 96, 3)\n",
        "  x = _reduce_conv(x, 128, 3)\n",
        "  x = _context_conv(x, 128, 3)\n",
        "  x = _reduce_conv(x, 160, 3)\n",
        "  x = _context_conv(x, 160, 3)\n",
        "  x = _reduce_conv(x, 192, 3)\n",
        "  x = _context_conv(x, 192, 3)\n",
        "  x = _reduce_conv(x, 256, 3)\n",
        "  x = _context_conv(x, 256, 3)\n",
        "\n",
        "  x = Dropout(0.3)(x)\n",
        "  x = Conv1D(num_classes, 5, activation='softmax')(x)\n",
        "  x = Reshape([-1])(x)\n",
        "\n",
        "  model = Model(input_layer, x, name='conv_1d_time_stacked')\n",
        "  model.compile(\n",
        "      optimizer=keras.optimizers.Adam(lr=3e-4),\n",
        "      loss=keras.losses.categorical_crossentropy,\n",
        "      metrics=[keras.metrics.categorical_accuracy])\n",
        "  return model\n",
        "\n",
        "\n",
        "def speech_model(model_type, input_size, num_classes=11, *args, **kwargs):\n",
        "  if model_type == 'conv_1d_time_stacked':\n",
        "    return conv_1d_time_stacked_model(input_size, num_classes)\n",
        "  else:\n",
        "    raise ValueError('Invalid model: %s' % model_type)\n",
        "\n",
        "\n",
        "def prepare_model_settings(label_count,\n",
        "                           sample_rate,\n",
        "                           clip_duration_ms,\n",
        "                           window_size_ms,\n",
        "                           window_stride_ms,\n",
        "                           dct_coefficient_count,\n",
        "                           num_log_mel_features,\n",
        "                           output_representation='raw'):\n",
        "  \"\"\"Calculates common settings needed for all models.\"\"\"\n",
        "  desired_samples = int(sample_rate * clip_duration_ms / 1000)\n",
        "  window_size_samples = int(sample_rate * window_size_ms / 1000)\n",
        "  window_stride_samples = int(sample_rate * window_stride_ms / 1000)\n",
        "  length_minus_window = (desired_samples - window_size_samples)\n",
        "  spectrogram_frequencies = 257\n",
        "  if length_minus_window < 0:\n",
        "    spectrogram_length = 0\n",
        "  else:\n",
        "    spectrogram_length = 1 + int(length_minus_window / window_stride_samples)\n",
        "\n",
        "  if output_representation == 'mfcc':\n",
        "    fingerprint_size = num_log_mel_features * spectrogram_length\n",
        "  elif output_representation == 'raw':\n",
        "    fingerprint_size = desired_samples\n",
        "  elif output_representation == 'spec':\n",
        "    fingerprint_size = spectrogram_frequencies * spectrogram_length\n",
        "  elif output_representation == 'mfcc_and_raw':\n",
        "    fingerprint_size = num_log_mel_features * spectrogram_length\n",
        "  return {\n",
        "      'desired_samples': desired_samples,\n",
        "      'window_size_samples': window_size_samples,\n",
        "      'window_stride_samples': window_stride_samples,\n",
        "      'spectrogram_length': spectrogram_length,\n",
        "      'spectrogram_frequencies': spectrogram_frequencies,\n",
        "      'dct_coefficient_count': dct_coefficient_count,\n",
        "      'fingerprint_size': fingerprint_size,\n",
        "      'label_count': label_count,\n",
        "      'sample_rate': sample_rate,\n",
        "      'num_log_mel_features': num_log_mel_features\n",
        "  }\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-4d7c7b1d83a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mPreprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mPreprocessRaw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/layers/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, function, output_shape, mask, arguments, **kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m     def __init__(self, function, output_shape=None,\n\u001b[1;32m    632\u001b[0m                  mask=None, arguments=None, **kwargs):\n\u001b[0;32m--> 633\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marguments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marguments\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0marguments\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_to_snake_case\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_uid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_uid\u001b[0;34m(prefix)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \"\"\"\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'get_default_graph'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nLX3hmEgxPu",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSc3NcaJgAfr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "8dda0fbd-173c-4f3d-d471-211ad9a3b854"
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "from keras import backend as K\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "parser = argparse.ArgumentParser(description='set input arguments')\n",
        "\n",
        "parser.add_argument(\n",
        "    '-sample_rate',\n",
        "    action='store',\n",
        "    dest='sample_rate',\n",
        "    type=int,\n",
        "    default=16000,\n",
        "    help='Sample rate of audio')\n",
        "parser.add_argument(\n",
        "    '-batch_size',\n",
        "    action='store',\n",
        "    dest='batch_size',\n",
        "    type=int,\n",
        "    default=32,\n",
        "    help='Size of the training batch')\n",
        "parser.add_argument(\n",
        "    '-output_representation',\n",
        "    action='store',\n",
        "    dest='output_representation',\n",
        "    type=str,\n",
        "    default='raw',\n",
        "    help='raw, spec, mfcc or mfcc_and_raw')\n",
        "parser.add_argument(\n",
        "    '-data_dirs',\n",
        "    '--list',\n",
        "    dest='data_dirs',\n",
        "    nargs='+',\n",
        "    required=True,\n",
        "    help='<Required> The list of data directories. e.g., data/train')\n",
        "\n",
        "args = parser.parse_args()\n",
        "parser.print_help()\n",
        "print('input args: ', args)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=1.0)\n",
        "  sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
        "  K.set_session(sess)\n",
        "  data_dirs = args.data_dirs\n",
        "  output_representation = args.output_representation\n",
        "  sample_rate = args.sample_rate\n",
        "  batch_size = args.batch_size\n",
        "  classes = get_classes(wanted_only=True)\n",
        "  model_settings = prepare_model_settings(\n",
        "      label_count=len(prepare_words_list(classes)),\n",
        "      sample_rate=sample_rate,\n",
        "      clip_duration_ms=1000,\n",
        "      window_size_ms=30.0,\n",
        "      window_stride_ms=10.0,\n",
        "      dct_coefficient_count=80,\n",
        "      num_log_mel_features=60,\n",
        "      output_representation=output_representation)\n",
        "\n",
        "  print(model_settings)\n",
        "\n",
        "  ap = AudioProcessor(\n",
        "      data_dirs=data_dirs,\n",
        "      wanted_words=classes,\n",
        "      silence_percentage=13.0,\n",
        "      unknown_percentage=60.0,\n",
        "      validation_percentage=10.0,\n",
        "      testing_percentage=0.0,\n",
        "      model_settings=model_settings,\n",
        "      output_representation=output_representation)\n",
        "  train_gen = data_gen(ap, sess, batch_size=batch_size, mode='training')\n",
        "  val_gen = data_gen(ap, sess, batch_size=batch_size, mode='validation')\n",
        "\n",
        "  model = speech_model(\n",
        "      'conv_1d_time_stacked',\n",
        "      model_settings['fingerprint_size']\n",
        "      if output_representation != 'raw' else model_settings['desired_samples'],\n",
        "      # noqa\n",
        "      num_classes=model_settings['label_count'],\n",
        "      **model_settings)\n",
        "\n",
        "  # embed()\n",
        "  checkpoints_path = os.path.join('checkpoints', 'conv_1d_time_stacked_model')\n",
        "  if not os.path.exists(checkpoints_path):\n",
        "    os.makedirs(checkpoints_path)\n",
        "\n",
        "  callbacks = [\n",
        "      ConfusionMatrixCallback(\n",
        "          val_gen,\n",
        "          ap.set_size('validation') // batch_size,\n",
        "          wanted_words=prepare_words_list(get_classes(wanted_only=True)),\n",
        "          all_words=prepare_words_list(classes),\n",
        "          label2int=ap.word_to_index),\n",
        "      ReduceLROnPlateau(\n",
        "          monitor='val_categorical_accuracy',\n",
        "          mode='max',\n",
        "          factor=0.5,\n",
        "          patience=4,\n",
        "          verbose=1,\n",
        "          min_lr=1e-5),\n",
        "      TensorBoard(log_dir='logs'),\n",
        "      ModelCheckpoint(\n",
        "          os.path.join(checkpoints_path,\n",
        "                       'ep-{epoch:03d}-vl-{val_loss:.4f}.hdf5'),\n",
        "          save_best_only=True,\n",
        "          monitor='val_categorical_accuracy',\n",
        "          mode='max')\n",
        "  ]\n",
        "  model.fit_generator(\n",
        "      train_gen,\n",
        "      steps_per_epoch=ap.set_size('training') // batch_size,\n",
        "      epochs=100,\n",
        "      verbose=1,\n",
        "      callbacks=callbacks)\n",
        "\n",
        "  eval_res = model.evaluate_generator(val_gen,\n",
        "                                      ap.set_size('validation') // batch_size)\n",
        "  print(eval_res)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: ipykernel_launcher.py [-h] [-sample_rate SAMPLE_RATE]\n",
            "                             [-batch_size BATCH_SIZE]\n",
            "                             [-output_representation OUTPUT_REPRESENTATION]\n",
            "                             -data_dirs DATA_DIRS [DATA_DIRS ...]\n",
            "ipykernel_launcher.py: error: the following arguments are required: -data_dirs/--list\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}